{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ripser"
      ],
      "metadata": {
        "id": "2yr7q7FtGl8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V9ZrPLB-EWaG"
      },
      "outputs": [],
      "source": [
        "# Package Import\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans, SpectralClustering, MeanShift, estimate_bandwidth, DBSCAN, OPTICS, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.datasets import make_moons, make_blobs\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, jaccard_score, rand_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial import distance_matrix\n",
        "import networkx as nx\n",
        "from ripser import ripser\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"src/main/resources/datasets/artificial/3MC.arff\"\"\"\n",
        "\n",
        "# # Generate synthetic data with 3 clusters from clustering benchmark\n",
        "import numpy as np\n",
        "data = np.loadtxt('3mc1.txt')\n",
        "true_labels = np.loadtxt('3mc_label.txt')\n",
        "data_df = pd.DataFrame(data, columns=['x', 'y'])\n",
        "data_df['labels'] = true_labels\n",
        "plt.scatter(data_df['x'], data_df['y'],c= data_df['labels'], cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QjHMNKBYEXiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BNFC"
      ],
      "metadata": {
        "id": "sDKCq-hrE5DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ripser import ripser\n",
        "\n",
        "num_clust = int(input(\"Number of Clusters:\"))\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def extract_betti(PD, scale_seq):\n",
        "    b = np.zeros(len(scale_seq))\n",
        "    for k in range(len(scale_seq)):\n",
        "        b[k] = np.sum((scale_seq[k] >= PD[:, 0]) & (scale_seq[k] < PD[:, 1]))\n",
        "    return b\n",
        "\n",
        "def weighted_knn(data, nKNN):\n",
        "    knn = NearestNeighbors(n_neighbors=nKNN).fit(data)\n",
        "    dists, ind = knn.kneighbors(data)\n",
        "\n",
        "    # Compute weights inversely proportional to distances\n",
        "    weights = 1 / (dists + 1e-5)  # Adding a small value to avoid division by zero\n",
        "\n",
        "    return ind, weights\n",
        "\n",
        "def bnfc_weighted(data, nKNN, filt_len=100):\n",
        "    N = data.shape[0]\n",
        "\n",
        "    # Perform weighted k-NN\n",
        "    ind, weights = weighted_knn(data, nKNN)  # Get indices and weights from weighted knn\n",
        "    maxscale = np.max(1 / weights[:, -1])  # Max of inverse of weights (distances)\n",
        "\n",
        "    scale_seq = np.linspace(0, maxscale, filt_len)\n",
        "\n",
        "    # Initialize arrays to store Betti numbers\n",
        "    betti_0 = np.zeros((N, filt_len))\n",
        "    betti_1 = np.zeros((N, filt_len))\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        neighborhood = data[ind[i,]]\n",
        "\n",
        "        # Compute persistent homology\n",
        "        rips = ripser(neighborhood, maxdim=1)\n",
        "        PD_0 = rips['dgms'][0]\n",
        "        PD_1 = rips['dgms'][1]\n",
        "\n",
        "        PD_0[PD_0[:, 1] == np.inf, 1] = maxscale\n",
        "        PD_1[PD_1[:, 1] == np.inf, 1] = maxscale\n",
        "\n",
        "        betti_0[i, :] = extract_betti(PD_0, scale_seq)\n",
        "        betti_1[i, :] = extract_betti(PD_1, scale_seq)\n",
        "\n",
        "    cos_scores_0= np.zeros((N, nKNN))\n",
        "    cos_scores_1= np.zeros((N, nKNN))\n",
        "\n",
        "    # Compute cosine similarity between data points based on their Betti numbers\n",
        "    for i in range(N):\n",
        "        current_betti_0 = betti_0[i, :].reshape(1, -1)\n",
        "        current_betti_1 = betti_1[i, :].reshape(1, -1)\n",
        "\n",
        "        neighbors_betti_0 = betti_0[ind[i,], :]\n",
        "        neighbors_betti_1 = betti_1[ind[i,], :]\n",
        "\n",
        "        sim_0 = cosine_similarity(current_betti_0, neighbors_betti_0).flatten()\n",
        "        cos_scores_0[i,:]= sim_0\n",
        "\n",
        "        sim_1 = cosine_similarity(current_betti_1, neighbors_betti_1).flatten()\n",
        "        cos_scores_1[i,:]= sim_1\n",
        "\n",
        "    # Initialize adjacency matrix\n",
        "    A = np.zeros((N, N))\n",
        "\n",
        "    tao_1 = np.percentile(cos_scores_0, 75) + 1.5 * (np.percentile(cos_scores_0, 75) - np.percentile(cos_scores_0, 25))\n",
        "    tao_0 = np.percentile(cos_scores_0, 25) - 1.5 * (np.percentile(cos_scores_0, 75) - np.percentile(cos_scores_0, 25))\n",
        "    meu_1 = np.percentile(cos_scores_1, 75) + 1.5 * (np.percentile(cos_scores_1, 75) - np.percentile(cos_scores_1, 25))\n",
        "    meu_0 = np.percentile(cos_scores_1, 25) - 1.5 * (np.percentile(cos_scores_1, 75) - np.percentile(cos_scores_1, 25))\n",
        "\n",
        "    for i in range(N):\n",
        "        indices_1 = ind[i, cos_scores_0[i,] >= tao_0]\n",
        "        indices_2 = ind[i, cos_scores_1[i,] >= meu_0]\n",
        "\n",
        "        common_indices = np.intersect1d(indices_1, indices_2)\n",
        "\n",
        "        # Update adjacency matrix by connecting similar points\n",
        "        A[i, common_indices] = 1\n",
        "\n",
        "    return A\n",
        "\n",
        "from numpy.linalg import eig\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "def tpspect(data, nKNN):\n",
        "\n",
        "  def calculate_U(X, V):\n",
        "      n, d = X.shape\n",
        "      U = np.zeros(n)\n",
        "      for i in range(n):\n",
        "          dist = ((X[i] - V) * (X[i] - V)).sum(1)\n",
        "          j = dist.argmin()\n",
        "          U[i] =j\n",
        "      return U\n",
        "\n",
        "  def calculate_V(X, U, k):\n",
        "      n, d = X.shape\n",
        "      V = np.zeros([k, d])\n",
        "      for j in range(k):\n",
        "          index = np.where(U == j)[0]\n",
        "          r = index.shape[0]\n",
        "          if r == 0:\n",
        "              V[j] = rng.choice(X)\n",
        "          else:\n",
        "              for i in index:\n",
        "                  V[j] = V[j] + X[i]\n",
        "              V[j] = V[j] / r\n",
        "      return V\n",
        "\n",
        "  def k_means(X, k):\n",
        "      n, d = X.shape\n",
        "      U = np.zeros(n)\n",
        "      V = rng.choice(X, k)\n",
        "      condition = True\n",
        "      while (condition):\n",
        "          temp = np.copy(U)\n",
        "          U = calculate_U(X, V)\n",
        "          V = calculate_V(X, U, k)\n",
        "          condition = not np.array_equal(U, temp)\n",
        "      return U\n",
        "\n",
        "  k = num_clust\n",
        "  n = data.shape[0]\n",
        "\n",
        "  A = bnfc_weighted(data, nKNN)\n",
        "\n",
        "  Adj = np.zeros((n, n))\n",
        "  sigma = np.std(data)\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if A[i, j]==1:\n",
        "        Adj[i, j] = np.exp(-np.linalg.norm(data[i] - data[j])**2 / (2*(sigma**2)))*A[i,j]*A[j,i]\n",
        "\n",
        "  D = np.diag(np.sum(Adj, axis=1))\n",
        "\n",
        "  L = D - Adj\n",
        "  eigenvalues, eigenvectors = eig(L)\n",
        "\n",
        "  indices = np.argsort(eigenvalues)[:k]\n",
        "  U = eigenvectors[:,indices]\n",
        "\n",
        "  bnfc_label = k_means(U,k)\n",
        "\n",
        "  return bnfc_label"
      ],
      "metadata": {
        "id": "14nEoloSEXdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store ARI scores\n",
        "ari_scores = []\n",
        "\n",
        "nKNN_values = range(1, 15)\n",
        "\n",
        "# Perform clustering and compute ARI for each nKNN\n",
        "for nKNN in nKNN_values:\n",
        "    Tplabel = tpspect(data, nKNN)\n",
        "    ari = adjusted_rand_score(true_labels, Tplabel)\n",
        "    ari_scores.append(ari)\n",
        "\n",
        "# # Plot the ARI scores\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(nKNN_values, ari_scores, marker='o')\n",
        "# plt.xlabel('nKNN')\n",
        "# plt.ylabel('Adjusted Rand Index (ARI)')\n",
        "# plt.title('ARI for Different nKNN Values')\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "nKNN = np.argmax(ari_scores)+1\n",
        "bnfc_label = tpspect(data, nKNN)\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['x', 'y'])\n",
        "data_df['labels'] = bnfc_label\n",
        "plt.scatter(data_df['x'], data_df['y'],c= data_df['labels'], cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TGdEPaPWEXcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnfc_ari = adjusted_rand_score(true_labels, bnfc_label)\n",
        "bnfc_ri = rand_score(true_labels, bnfc_label)\n",
        "bnfc_nmi = normalized_mutual_info_score(true_labels, bnfc_label)\n",
        "\n",
        "print(f'BNFC RI: {bnfc_ri:.3f}, BNFC ARI: {bnfc_ari:.3f}, NMI: {bnfc_nmi: .3f}')"
      ],
      "metadata": {
        "id": "EukGazK8EXZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}