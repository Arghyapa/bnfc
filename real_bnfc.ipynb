{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUAB766iuuJ5"
      },
      "outputs": [],
      "source": [
        "!pip install ripser\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial import distance_matrix\n",
        "import networkx as nx\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, jaccard_score, rand_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans, SpectralClustering, MeanShift, estimate_bandwidth, DBSCAN, OPTICS, AgglomerativeClustering\n",
        "\n",
        "import networkx as nx\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from itertools import combinations\n",
        "\n",
        "label = np.loadtxt('Glass-label.txt')\n",
        "data = np.loadtxt('glass1.txt')\n",
        "\n",
        "from ripser import ripser\n",
        "\n",
        "num_clust = int(input(\"Number of Clusters:\"))\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def extract_betti(PD, scale_seq):\n",
        "    b = np.zeros(len(scale_seq))\n",
        "    for k in range(len(scale_seq)):\n",
        "        b[k] = np.sum((scale_seq[k] >= PD[:, 0]) & (scale_seq[k] < PD[:, 1]))\n",
        "    return b\n",
        "\n",
        "def weighted_knn(data, nKNN):\n",
        "    knn = NearestNeighbors(n_neighbors=nKNN).fit(data)\n",
        "    dists, ind = knn.kneighbors(data)\n",
        "\n",
        "    # Compute weights inversely proportional to distances\n",
        "    weights = 1 / (dists + 1e-5)  # Adding a small value to avoid division by zero\n",
        "\n",
        "    return ind, weights\n",
        "\n",
        "def bnfc_weighted(data, nKNN, filt_len=100):\n",
        "    N = data.shape[0]\n",
        "\n",
        "    # Perform weighted k-NN\n",
        "    ind, weights = weighted_knn(data, nKNN)\n",
        "    maxscale = np.max(1 / weights[:, -1])\n",
        "\n",
        "    scale_seq = np.linspace(0, maxscale, filt_len)\n",
        "\n",
        "\n",
        "    betti_0 = np.zeros((N, filt_len))\n",
        "    betti_1 = np.zeros((N, filt_len))\n",
        "    betti_2 = np.zeros((N, filt_len))\n",
        "\n",
        "    for i in range(N):\n",
        "\n",
        "        neighborhood = data[ind[i,]]\n",
        "\n",
        "        # Compute persistent homology\n",
        "        rips = ripser(neighborhood, maxdim=2)\n",
        "        PD_0 = rips['dgms'][0]\n",
        "        PD_1 = rips['dgms'][1]\n",
        "        PD_2 = rips['dgms'][2]\n",
        "\n",
        "        PD_0[PD_0[:, 1] == np.inf, 1] = maxscale\n",
        "        PD_1[PD_1[:, 1] == np.inf, 1] = maxscale\n",
        "        PD_2[PD_2[:, 1] == np.inf, 1] = maxscale\n",
        "\n",
        "        betti_0[i, :] = extract_betti(PD_0, scale_seq)\n",
        "        betti_1[i, :] = extract_betti(PD_1, scale_seq)\n",
        "        betti_2[i, :] = extract_betti(PD_2, scale_seq)\n",
        "\n",
        "    cos_scores_0= np.zeros((N, nKNN))\n",
        "    cos_scores_1= np.zeros((N, nKNN))\n",
        "    cos_scores_2= np.zeros((N, nKNN))\n",
        "\n",
        "    # Compute cosine similarity between data points based on their Betti numbers\n",
        "    for i in range(N):\n",
        "        current_betti_0 = betti_0[i, :].reshape(1, -1)\n",
        "        current_betti_1 = betti_1[i, :].reshape(1, -1)\n",
        "        current_betti_2 = betti_2[i, :].reshape(1, -1)\n",
        "\n",
        "        neighbors_betti_0 = betti_0[ind[i,], :]\n",
        "        neighbors_betti_1 = betti_1[ind[i,], :]\n",
        "        neighbors_betti_2 = betti_2[ind[i,], :]\n",
        "\n",
        "        sim_0 = cosine_similarity(current_betti_0, neighbors_betti_0).flatten()\n",
        "        cos_scores_0[i,:]= sim_0\n",
        "\n",
        "        sim_1 = cosine_similarity(current_betti_1, neighbors_betti_1).flatten()\n",
        "        cos_scores_1[i,:]= sim_1\n",
        "\n",
        "        sim_2 = cosine_similarity(current_betti_2, neighbors_betti_2).flatten()\n",
        "        cos_scores_2[i,:]= sim_2\n",
        "\n",
        "    A = np.zeros((N, N))\n",
        "\n",
        "    tao_0 = np.percentile(cos_scores_0, 25) - 1.5 * (np.percentile(cos_scores_0, 75) - np.percentile(cos_scores_0, 25))\n",
        "    meu_0 = np.percentile(cos_scores_1, 25) - 1.5 * (np.percentile(cos_scores_1, 75) - np.percentile(cos_scores_1, 25))\n",
        "    gamma_0 = np.percentile(cos_scores_2, 25) - 1.5 * (np.percentile(cos_scores_2, 75) - np.percentile(cos_scores_2, 25))\n",
        "\n",
        "    for i in range(N):\n",
        "        indices_1 = ind[i, cos_scores_0[i,] >= tao_0]\n",
        "        indices_2 = ind[i, cos_scores_1[i,] >= meu_0]\n",
        "        indices_3 = ind[i, cos_scores_2[i,] >= gamma_0]\n",
        "\n",
        "        common_indices = np.intersect1d(np.intersect1d(indices_1, indices_2), indices_3)\n",
        "\n",
        "        A[i, common_indices] = 1\n",
        "\n",
        "    return A\n",
        "\n",
        "from numpy.linalg import eig\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "def tpspect(data, nKNN):\n",
        "\n",
        "  def calculate_U(X, V):\n",
        "      n, d = X.shape\n",
        "      U = np.zeros(n)\n",
        "      for i in range(n):\n",
        "          dist = ((X[i] - V) * (X[i] - V)).sum(1)\n",
        "          j = dist.argmin()\n",
        "          U[i] =j\n",
        "      return U\n",
        "\n",
        "  def calculate_V(X, U, k):\n",
        "      n, d = X.shape\n",
        "      V = np.zeros([k, d])\n",
        "      for j in range(k):\n",
        "          index = np.where(U == j)[0]\n",
        "          r = index.shape[0]\n",
        "          if r == 0:\n",
        "              V[j] = rng.choice(X)\n",
        "          else:\n",
        "              for i in index:\n",
        "                  V[j] = V[j] + X[i]\n",
        "              V[j] = V[j] / r\n",
        "      return V\n",
        "\n",
        "  def k_means(X, k):\n",
        "      n, d = X.shape\n",
        "      U = np.zeros(n)\n",
        "      V = rng.choice(X, k)\n",
        "      condition = True\n",
        "      while (condition):\n",
        "          temp = np.copy(U)\n",
        "          U = calculate_U(X, V)\n",
        "          V = calculate_V(X, U, k)\n",
        "          condition = not np.array_equal(U, temp)\n",
        "      return U\n",
        "\n",
        "  k = num_clust\n",
        "  n = data.shape[0]\n",
        "\n",
        "  A = bnfc_weighted(data, nKNN)\n",
        "\n",
        "  Adj = np.zeros((n, n))\n",
        "  sigma = np.std(data)\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if A[i, j]==1:\n",
        "        Adj[i, j] = np.exp(-np.linalg.norm(data[i] - data[j])**2 / (2*(sigma**2)))*A[i,j]*A[j,i]\n",
        "\n",
        "  D = np.diag(np.sum(Adj, axis=1))\n",
        "\n",
        "  L = D - Adj\n",
        "  eigenvalues, eigenvectors = eig(L)\n",
        "\n",
        "  indices = np.argsort(eigenvalues)[:k]\n",
        "  U = eigenvectors[:,indices]\n",
        "\n",
        "  bnfc_label = k_means(U,k)\n",
        "\n",
        "  return bnfc_label\n",
        "\n",
        "\n",
        "ari_scores = []\n",
        "\n",
        "nKNN_values = range(1, 15)\n",
        "\n",
        "for nKNN in nKNN_values:\n",
        "    Tplabel = tpspect(data, nKNN)\n",
        "    ari = adjusted_rand_score(label, Tplabel)\n",
        "    ari_scores.append(ari)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(nKNN_values, ari_scores, marker='o')\n",
        "plt.xlabel('nKNN')\n",
        "plt.ylabel('Adjusted Rand Index (ARI)')\n",
        "plt.title('ARI for Different nKNN Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# KNN = int(input(\"KNN:\"))\n",
        "KNN = np.argmax(ari_scores)+1\n",
        "bnfc_label = tpspect(data, nKNN=KNN)\n",
        "\n",
        "bnfc_ari = adjusted_rand_score(label, bnfc_label)\n",
        "bnfc_ri = rand_score(label, bnfc_label)\n",
        "bnfc_nmi = normalized_mutual_info_score(label, bnfc_label)\n",
        "\n",
        "print(f'BNFC RI: {bnfc_ri:.3f}, BNFC ARI: {bnfc_ari:.3f}, NMI: {bnfc_nmi: .3f}')\n"
      ]
    }
  ]
}